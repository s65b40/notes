

## NSP 做 zero-shot
传统的: token-level 任务是生成对应的token
NSP：sentence-level 任务是判断把prompt作为第二句，判断第一句与哪个prompt更为连贯,prompt是标签句子

## PET 
Pattern-Exploiting Training

## P-tuning
P-tuning 连续型prompt
与加个全连接层fine-tuning的异同
> 预训练模型可以认为是语言模型任务
>
> 下游任务可以表示为该种语言模型的某个特殊情形
>
> 当输出空间有限时，它又近似于加一个全连接层作为语言模型
> 
>所以加一个全连接层有效
P-tuning可以视为加在模型中的参数


# 想法
离散型prompt的优化
* 元学习：将prompt作为参数,在元学习上进行初始化
* 
